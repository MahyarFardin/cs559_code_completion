{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS559 Code Completion (Colab Notebook) — Token-Level Only\n",
        "\n",
        "This notebook runs the full pipeline end-to-end for **token-level next-token prediction**:\n",
        "\n",
        "1. Install dependencies\n",
        "2. Download + extract Py150\n",
        "3. Preprocess/tokenize\n",
        "4. Create **token-level** completion datasets\n",
        "5. Train (`train_v2.py`)\n",
        "6. Evaluate (`evaluate.py`)\n",
        "7. Run a small inference demo\n",
        "\n",
        "**Tip:** start with the **Quick sanity run** settings (small `max_train_examples`) to verify everything works, then scale up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) Mount Google Drive to save runs\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print('Not running in Colab (or Drive mount failed):', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repo (if you uploaded only this notebook)\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "REPO_URL = \"https://github.com/MahyarFardin/cs559_code_completion.git\"\n",
        "REPO_DIR = \"/content/cs559_code_completion\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    subprocess.check_call([\"git\", \"clone\", REPO_URL, REPO_DIR])\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"cwd:\", os.getcwd())\n",
        "subprocess.check_call([\"python\", \"-V\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# Note: Colab typically already includes PyTorch; requirements.txt should be compatible.\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "print('torch:', torch.__version__)\n",
        "print('cuda available:', torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        subprocess.check_call([\"nvidia-smi\"])\n",
        "    except Exception as e:\n",
        "        print(\"nvidia-smi failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure your run\n",
        "\n",
        "- Set `QUICK_RUN=True` first to validate the pipeline.\n",
        "- When you scale up, increase `MAX_TRAIN_EXAMPLES` and optionally `NUM_EPOCHS`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Run configuration (token-level only) ---\n",
        "QUICK_RUN = True\n",
        "\n",
        "# Fixed task for this notebook\n",
        "TASK = 'token'\n",
        "\n",
        "# If True, rebuild cached dataset JSONLs (slow)\n",
        "FORCE_REBUILD_DATASET = False\n",
        "\n",
        "# Common training hyperparams\n",
        "BATCH_SIZE = 16\n",
        "ACCUMULATION_STEPS = 4\n",
        "NUM_EPOCHS = 2 if QUICK_RUN else 15\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "# Vocab controls model size heavily\n",
        "VOCAB_MIN_FREQ = 50 if QUICK_RUN else 20\n",
        "VOCAB_SAMPLE_LINES = 20000 if QUICK_RUN else 50000\n",
        "\n",
        "# Dataset sizing\n",
        "MAX_TRAIN_EXAMPLES = 2000 if QUICK_RUN else 200000\n",
        "MAX_VAL_EXAMPLES = 1000 if QUICK_RUN else 10000\n",
        "MAX_TEST_EXAMPLES = 2000 if QUICK_RUN else 50000\n",
        "\n",
        "# DataLoader workers\n",
        "NUM_WORKERS = 2\n",
        "EVAL_NUM_WORKERS = 0  # set 0 if evaluation hangs\n",
        "\n",
        "# Device\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def make_run_name(task, bs, ep, max_length, vocab_min_freq, max_train_examples, acc_steps):\n",
        "    name = f\"run_{task}_v2_bs{bs}_ep{ep}_len{max_length}_vocab{vocab_min_freq}\"\n",
        "    if max_train_examples:\n",
        "        name += f\"_train{max_train_examples}\"\n",
        "    if acc_steps and acc_steps > 1:\n",
        "        name += f\"_acc{acc_steps}\"\n",
        "    return name\n",
        "\n",
        "RUN_NAME = make_run_name(TASK, BATCH_SIZE, NUM_EPOCHS, MAX_LENGTH, VOCAB_MIN_FREQ, MAX_TRAIN_EXAMPLES, ACCUMULATION_STEPS)\n",
        "OUTPUT_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Shared dataset cache (one-time creation; reused across runs)\n",
        "# - On Colab: lives under /content\n",
        "# - Locally: lives under the current repo directory\n",
        "COMMON_DATASET_ROOT = \"/content/cs559_shared_datasets\" if os.path.exists(\"/content\") else os.path.join(os.getcwd(), \"cs559_shared_datasets\")\n",
        "DATASET_DIR = os.path.join(COMMON_DATASET_ROOT, \"completion_datasets_token_only\")\n",
        "\n",
        "MODEL_PATH = f\"{OUTPUT_DIR}/best_model_{TASK}_level.pt\"\n",
        "VOCAB_PATH = f\"{OUTPUT_DIR}/vocab.json\"\n",
        "\n",
        "print('TASK:', TASK)\n",
        "print('RUN_NAME:', RUN_NAME)\n",
        "print('OUTPUT_DIR:', OUTPUT_DIR)\n",
        "print('DATASET_DIR:', DATASET_DIR)\n",
        "print('FORCE_REBUILD_DATASET:', FORCE_REBUILD_DATASET)\n",
        "print('MODEL_PATH:', MODEL_PATH)\n",
        "print('VOCAB_PATH:', VOCAB_PATH)\n",
        "print('DEVICE:', DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download dataset (Py150)\n",
        "\n",
        "This can take a while. If you already have `py150_files/` from a previous session, you can skip.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "subprocess.check_call([\"bash\", \"download_and_extract.sh\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess/tokenize\n",
        "\n",
        "Creates `token_completion/{train,dev,test}.txt`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python preprocess.py --base_dir py150_files --output_dir token_completion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create completion datasets (token-level only)\n",
        "\n",
        "Creates `{DATASET_DIR}/token_level/{train,dev,test}.jsonl` (stored in a shared cache folder so you don’t regenerate it every run).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5326a57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate token-level dataset only (shared cache)\n",
        "# This runs once and then reuses the cached JSONLs across runs.\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "token_train = os.path.join(DATASET_DIR, \"token_level\", \"train.jsonl\")\n",
        "\n",
        "if FORCE_REBUILD_DATASET or not os.path.exists(token_train):\n",
        "    print(\"Building token-level completion dataset cache...\")\n",
        "    shutil.rmtree(DATASET_DIR, ignore_errors=True)\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "    !python create_completion_datasets.py --input_dir token_completion --output_dir {DATASET_DIR} --token_level\n",
        "else:\n",
        "    print(\"Found cached token-level dataset. Skipping rebuild:\", token_train)\n",
        "\n",
        "# Safety: if an old line_level folder exists in the cache, remove it\n",
        "line_level_dir = os.path.join(DATASET_DIR, \"line_level\")\n",
        "if os.path.exists(line_level_dir):\n",
        "    print(\"Removing stale line_level cache:\", line_level_dir)\n",
        "    shutil.rmtree(line_level_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train token-level (train_v2.py)\n",
        "\n",
        "- This creates `runs/<RUN_NAME>/` with `training_params.json`, `vocab.json`, and the best model checkpoint.\n",
        "- Note: your `train_v2.py` run naming is **deterministic** (no timestamp). If you re-run the same config, it will reuse the same folder and can resume from checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_v2.py \\\n",
        "  --task {TASK} \\\n",
        "  --dataset_dir {DATASET_DIR} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --accumulation_steps {ACCUMULATION_STEPS} \\\n",
        "  --num_epochs {NUM_EPOCHS} \\\n",
        "  --max_length {MAX_LENGTH} \\\n",
        "  --vocab_min_freq {VOCAB_MIN_FREQ} \\\n",
        "  --vocab_sample_lines {VOCAB_SAMPLE_LINES} \\\n",
        "  --max_train_examples {MAX_TRAIN_EXAMPLES} \\\n",
        "  --max_val_examples {MAX_VAL_EXAMPLES} \\\n",
        "  --lazy_load \\\n",
        "  --num_workers {NUM_WORKERS} \\\n",
        "  --device {DEVICE}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "- Uses the model from the run directory.\n",
        "- If evaluation hangs at `0%`, keep `--num_workers 0`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782885cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python evaluate.py \\\n",
        "  --model_path {MODEL_PATH} \\\n",
        "  --vocab_path {VOCAB_PATH} \\\n",
        "  --task {TASK} \\\n",
        "  --dataset_dir {DATASET_DIR} \\\n",
        "  --max_length {MAX_LENGTH} \\\n",
        "  --max_test_examples {MAX_TEST_EXAMPLES} \\\n",
        "  --lazy_load \\\n",
        "  --num_workers {EVAL_NUM_WORKERS} \\\n",
        "  --device {DEVICE}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick token-level inference demo\n",
        "\n",
        "Adjust the `--context` string to try different prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "045b8f67",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Token-level inference\n",
        "cmd = (\n",
        "    f\"python inference.py \"\n",
        "    f\"--model_path {MODEL_PATH} \"\n",
        "    f\"--vocab_path {VOCAB_PATH} \"\n",
        "    f\"--task token \"\n",
        "    f\"--context \\\"from collections import\\\" \"\n",
        "    f\"--top_k 5 \"\n",
        "    f\"--device {DEVICE}\"\n",
        ")\n",
        "\n",
        "print(cmd)\n",
        "subprocess.check_call(cmd, shell=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save your run to Google Drive (optional)\n",
        "\n",
        "If you mounted Drive, copy the whole run folder so it persists after the Colab session ends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "drive_root = '/content/drive/MyDrive'\n",
        "if os.path.exists('/content/drive'):\n",
        "    dst = os.path.join(drive_root, 'cs559_runs', RUN_NAME)\n",
        "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "    if os.path.exists(dst):\n",
        "        print('Destination already exists, removing:', dst)\n",
        "        shutil.rmtree(dst)\n",
        "    shutil.copytree(OUTPUT_DIR, dst)\n",
        "    print('Copied run to:', dst)\n",
        "else:\n",
        "    print('Drive is not mounted; skipping copy.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
